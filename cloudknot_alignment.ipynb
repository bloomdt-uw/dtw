{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamline Aligned Bundle Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck\n",
    "\n",
    "ck.set_region('us-west-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_reliability(ak, sk):        \n",
    "    import os\n",
    "    import os.path as op\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    import s3fs\n",
    "    \n",
    "    from AFQ import api\n",
    "    import AFQ.data as afd\n",
    "    \n",
    "    from fastdtw import fastdtw\n",
    "    \n",
    "    import nibabel as nib\n",
    "    \n",
    "    from dipy.io.streamline import load_tractogram\n",
    "    from dipy.stats.analysis import afq_profile, gaussian_weights\n",
    "    from dipy.tracking.streamline import set_number_of_points, values_from_volume\n",
    "    \n",
    "    def get_hcp_afq(dataset_name):\n",
    "        afq = api.AFQ(\n",
    "            bids_path=op.join(afd.afq_home, dataset_name),\n",
    "            dmriprep='dmriprep'\n",
    "        )\n",
    "\n",
    "        return afq\n",
    "    \n",
    "    def get_subject_iloc(afq, subject):\n",
    "        iloc = afq.data_frame.index[afq.data_frame['subject'] == subject][0]\n",
    "\n",
    "        return iloc\n",
    "\n",
    "    def get_subject_scalar_data(afq, subject, scalar):\n",
    "        iloc = get_subject_iloc(afq, subject)\n",
    "\n",
    "        scalar_filename = afq._get_fname(\n",
    "            afq.data_frame.iloc[iloc],\n",
    "            f'_model-{scalar}.nii.gz'\n",
    "        )\n",
    "\n",
    "        scalar_data = nib.load(scalar_filename).get_fdata()\n",
    "\n",
    "        return scalar_data\n",
    "\n",
    "    def get_subject_bundle_tractogram(afq, subject, bundle_name):\n",
    "        iloc = get_subject_iloc(afq, subject)\n",
    "\n",
    "        results_dir = afq.data_frame.iloc[iloc]['results_dir']\n",
    "\n",
    "        fname = op.split(\n",
    "            afq._get_fname(\n",
    "                afq.data_frame.iloc[iloc],\n",
    "                f'-{bundle_name}'\n",
    "                f'_tractography.trk',\n",
    "                include_track=True,\n",
    "                include_seg=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        tractogram_filename = op.join(results_dir, 'clean_bundles', fname[1])\n",
    "\n",
    "        tractogram = load_tractogram(tractogram_filename, 'same')\n",
    "\n",
    "        return tractogram\n",
    "\n",
    "    def get_subject_bundle_profile(afq, subject, scalar_data, bundle_name):\n",
    "        tractogram = get_subject_bundle_tractogram(afq, subject, bundle_name)\n",
    "\n",
    "        if len(tractogram.streamlines) == 0:\n",
    "            return np.zeros(n_points)\n",
    "\n",
    "        profile = afq_profile(\n",
    "            scalar_data,\n",
    "            tractogram.streamlines,\n",
    "            tractogram.affine,\n",
    "            weights=gaussian_weights(tractogram.streamlines)\n",
    "        )\n",
    "\n",
    "        return profile\n",
    "\n",
    "    def get_bundle_profiles(afq):\n",
    "        bundle_profiles = {}\n",
    "\n",
    "        for subject in hcp_subjects:\n",
    "            bundle_profiles[subject] = {}\n",
    "\n",
    "            for scalar in afq.scalars:\n",
    "                bundle_profiles[subject][scalar] = {}\n",
    "                scalar_data = get_subject_scalar_data(afq, subject, scalar)\n",
    "\n",
    "                for bundle_name in bundle_names:\n",
    "                    bundle_profiles[subject][scalar][bundle_name] = get_subject_bundle_profile(afq, subject, scalar_data, bundle_name)\n",
    "\n",
    "        return bundle_profiles\n",
    "\n",
    "    def get_test_retest_correlations():\n",
    "        correlations = {}\n",
    "\n",
    "        test_bundle_profiles = get_bundle_profiles(hcp_test_afq)\n",
    "        retest_bundle_profiles = get_bundle_profiles(hcp_retest_afq)\n",
    "\n",
    "        for scalar in hcp_retest_afq.scalars:\n",
    "            correlations[scalar] = {}\n",
    "            for subject in hcp_subjects:\n",
    "                correlations[scalar][subject] = {}\n",
    "                for bundle_name in bundle_names:\n",
    "                    test_profile = test_bundle_profiles[subject][scalar][bundle_name]\n",
    "                    retest_profile = retest_bundle_profiles[subject][scalar][bundle_name]\n",
    "\n",
    "                    test_retest_corr_matrix = pd.DataFrame(zip(*[test_profile, retest_profile]), columns=['test', 'retest']).corr()\n",
    "\n",
    "                    # select only the upper triangle off diagonals of the correlation matrix\n",
    "                    test_retest_corr = test_retest_corr_matrix.where(np.triu(np.ones(test_retest_corr_matrix.shape), 1).astype(np.bool)).stack()\n",
    "\n",
    "                    if len(test_retest_corr) == 1:\n",
    "                        correlations[scalar][subject][bundle_name] = test_retest_corr[0]\n",
    "                    else:\n",
    "                        correlations[scalar][subject][bundle_name] = 0\n",
    "\n",
    "        return correlations\n",
    "    \n",
    "    def get_subject_mean_warped_bundle_profile(afq, subject, scalar_data, bundle_name):\n",
    "        tractogram = get_subject_bundle_tractogram(afq, subject, bundle_name)\n",
    "\n",
    "        if len(tractogram.streamlines) == 0:\n",
    "            return np.zeros(n_points)\n",
    "\n",
    "        fgarray = set_number_of_points(tractogram.streamlines, n_points)\n",
    "\n",
    "        values = np.array(values_from_volume(scalar_data, fgarray, tractogram.affine))\n",
    "        mean_values = np.mean(values, axis=0)\n",
    "\n",
    "        dtw_values = []\n",
    "\n",
    "        for value in values:\n",
    "            dist, path = fastdtw(value, mean_values)\n",
    "            path = np.array(path)\n",
    "            dtw_values.append(value[np.append(path[np.where(path[:,1][:-1] != path[:,1][1:]),0][0], len(values.T)-1)])\n",
    "\n",
    "        dtw_values = np.array(dtw_values)\n",
    "\n",
    "        dtw_mean_values = np.mean(dtw_values, axis=0)\n",
    "\n",
    "        return dtw_mean_values\n",
    "\n",
    "    def get_mean_warped_bundle_profiles(afq):\n",
    "        mean_warped_bundle_profiles = {}\n",
    "\n",
    "        for subject in hcp_subjects:\n",
    "            mean_warped_bundle_profiles[subject] = {}\n",
    "            iloc = get_subject_iloc(afq, subject)\n",
    "\n",
    "            for scalar in afq.scalars:\n",
    "                mean_warped_bundle_profiles[subject][scalar] = {}\n",
    "                scalar_data = get_subject_scalar_data(afq, subject, scalar)\n",
    "\n",
    "                for bundle_name in bundle_names:\n",
    "                    dtw_mean_values = get_subject_mean_warped_bundle_profile(afq, subject, scalar_data, bundle_name)\n",
    "                    mean_warped_bundle_profiles[subject][scalar][bundle_name] = dtw_mean_values\n",
    "\n",
    "        return mean_warped_bundle_profiles\n",
    "\n",
    "    def get_test_retest_warped_correlations():\n",
    "        correlations = {}\n",
    "\n",
    "        test_bundle_profiles = get_mean_warped_bundle_profiles(hcp_test_afq)\n",
    "        retest_bundle_profiles = get_mean_warped_bundle_profiles(hcp_retest_afq)\n",
    "\n",
    "        for scalar in hcp_retest_afq.scalars:\n",
    "            correlations[scalar] = {}\n",
    "            for subject in hcp_subjects:\n",
    "                correlations[scalar][subject] = {}\n",
    "                for bundle_name in bundle_names:\n",
    "                    test_profile = test_bundle_profiles[subject][scalar][bundle_name]\n",
    "                    retest_profile = retest_bundle_profiles[subject][scalar][bundle_name]\n",
    "\n",
    "                    test_retest_corr_matrix = pd.DataFrame(zip(*[test_profile, retest_profile]), columns=['test', 'retest']).corr()\n",
    "\n",
    "                    # select only the upper triangle off diagonals of the correlation matrix\n",
    "                    test_retest_corr = test_retest_corr_matrix.where(np.triu(np.ones(test_retest_corr_matrix.shape), 1).astype(np.bool)).stack()\n",
    "\n",
    "                    if len(test_retest_corr) == 1:\n",
    "                        correlations[scalar][subject][bundle_name] = test_retest_corr[0]\n",
    "                    else:\n",
    "                        correlations[scalar][subject][bundle_name] = 0\n",
    "\n",
    "        return correlations\n",
    "    \n",
    "    fs = s3fs.S3FileSystem(anon=False)\n",
    "    \n",
    "    hcp_subjects = [\n",
    "        '103818', '105923', '111312', '114823', '115320', '122317', '125525', '130518', '135528', '137128',\n",
    "        '139839', '143325', '144226', '146129', '149337', '149741', '151526', '158035', '169343', '172332',\n",
    "        '175439', '177746', '185442', '187547', '192439', '194140', '195041', '200109', '200614', '204521',\n",
    "        '250427', '287248', '341834', '433839', '562345', '599671', '601127', '627549', '660951', '662551',\n",
    "        '783462', '859671', '861456', '877168', '917255'\n",
    "    ]\n",
    "    \n",
    "    hcp_subjects = hcp_subjects[:2] # ['103818', '105923']\n",
    "    \n",
    "    afd.fetch_hcp(hcp_subjects,\n",
    "                  profile_name=False,\n",
    "                  aws_access_key_id=ak,\n",
    "                  aws_secret_access_key=sk)\n",
    "\n",
    "    afd.fetch_hcp(hcp_subjects, \n",
    "                  study='HCP_Retest', \n",
    "                  profile_name=False,\n",
    "                  aws_access_key_id=ak,\n",
    "                  aws_secret_access_key=sk)\n",
    "    \n",
    "    for subject in hcp_subjects:\n",
    "        # HCP test\n",
    "        subject_test_dir = op.join(afd.afq_home, 'HCP_1200', 'derivatives', 'afq', f'sub-{subject}')\n",
    "        os.makedirs(subject_test_dir)\n",
    "        fs.get(f'profile-hcp-west/hcp_reliability/single_shell/hcp_1200_afq/sub-{subject}/', subject_test_dir, recursive=True)\n",
    "\n",
    "\n",
    "        # HCP retest\n",
    "        subject_retest_dir = op.join(afd.afq_home, 'HCP_Retest', 'derivatives', 'afq', f'sub-{subject}')\n",
    "        os.makedirs(subject_retest_dir)\n",
    "        fs.get(f'profile-hcp-west/hcp_reliability/single_shell/hcp_retest_afq/sub-{subject}/', subject_retest_dir, recursive=True)\n",
    "    \n",
    "    # debugging why can't find data_description.json\n",
    "#     for root, dirs, files in os.walk(afd.afq_home):\n",
    "#         level = root.replace(afd.afq_home, '').count(os.sep)\n",
    "#         indent = ' ' * 4 * (level)\n",
    "#         print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "#         subindent = ' ' * 4 * (level + 1)\n",
    "#         for f in files:\n",
    "#             print('{}{}'.format(subindent, f))\n",
    "    \n",
    "    hcp_test_afq = get_hcp_afq('HCP_1200')\n",
    "    hcp_retest_afq = get_hcp_afq('HCP_Retest')\n",
    "    \n",
    "    bundle_names = [*hcp_retest_afq.bundle_dict]\n",
    "    n_points = 100\n",
    "    \n",
    "    corr = get_test_retest_correlations()\n",
    "    \n",
    "    with open('corr.pkl', 'wb') as handle:\n",
    "        pickle.dump(corr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    warped_corr = get_test_retest_warped_correlations()\n",
    "    \n",
    "    with open('warped_corr.pkl', 'wb') as handle:\n",
    "        pickle.dump(warped_corr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    fs.put('corr.pkl', 'warp-alignment/corr.pkl')\n",
    "    fs.put('warped_corr.pkl', 'warp-alignment/warped_corr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "knot = ck.Knot(\n",
    "    name=\"pro_rel-\" + datetime.now().isoformat()[:-7].replace(\":\",\"-\"),\n",
    "    pars_policies=(\"AmazonS3FullAccess\",),\n",
    "    base_image='python:3.8',\n",
    "    func=profile_reliability,\n",
    "    image_github_installs=\"https://github.com/yeatmanlab/pyAFQ.git\",\n",
    "#     image_github_installs=\"https://github.com/bloomdt-uw/pyAFQ.git@enh-565-subsample-streamlines\",\n",
    "    memory=32000,  # in MiB\n",
    "    volume_size=50,  # in GiB\n",
    "    bid_percentage=105\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get aws credentials to access HCP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os.path as op\n",
    "\n",
    "cp = configparser.ConfigParser()\n",
    "cp.read_file(open(op.join(op.expanduser('~'), '.aws', 'credentials')))\n",
    "cp.sections()\n",
    "ak = cp.get('hcp', 'AWS_ACCESS_KEY_ID')\n",
    "sk = cp.get('hcp', 'AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_futures = knot.map([(ak, sk)], starmap=True, job_type=\"independent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "059884e5-41e8-4d62-b37d-f24662ebdab5        pro-rel-2020-12-14T22-51-15-0        SUBMITTED\n"
     ]
    }
   ],
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber(clobber_pars=True, clobber_repo=True, clobber_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
